---
project:
  type: website
  output-dir: docs
title: "テキスト分析"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
format:
  html:
    toc: true
    toc-title: 目次
    toc_float: true
    toc-depth: 4
    number-sections: true
    theme: simplex
    mermaid:
      theme: neutral
lang: ja
---

テキスト分析は，大雑把には以下のような手順により，テキストデータを量的に分析する方法です。
なお，このページの説明には多くの間違いがある可能性があります。
間違いを指摘いただければ幸いです。

- テキストデータの収集
- テキストデータの読み込み
- 前処理
  + トークン化
  + 整理テキスト（tidy text）
  + 形態素解析
  + 単語文書行列（term-document matrix）または文書単語行列（document-term matrix）の作成
  + 文書特徴量行列（document-feature matrix）の作成
- 分析


## 前処理

テキスト分析では前処理が重要です。
前処理は，形態素解析の前に行います。

環境によっては文字コードを変換する必要がある場合もあります。
文字コードがUTF-8でない場合にUTF-8に変換するようにすればよいでしょう。
```{R}
#| eval: false
iconv(textdata, from = "CP932", to = "UTF-8")
```

表記揺れの修正は日本語のテキストマイニングにおいて重要でしょう。
例えば，次のようにします。
```{R}
#| eval: false
textdata <- gsub("子供", "子ども", textdata)
```

もう少し汎用的な処理は正規化と呼ばれる。
例えば，全角と半角を統一することが考えられます。
```{R}
#| eval: false
zenkaku <- "０１２３４５６７８９ａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺ"
hankaku <- "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"
hankana <- "｡｢｣､･ｦｧｨｩｪｫｬｭｮｯｰｱｲｳｴｵｶｷｸｹｺｻｼｽｾｿﾀﾁﾂﾃﾄﾅﾆﾇﾈﾉﾊﾋﾌﾍﾎﾏﾐﾑﾒﾓﾔﾕﾖﾗﾘﾙﾚﾛﾜﾝﾞﾟ"
zenkaku_kana <- "。「」、・ヲァィゥェォャュョッーアイウエオカキクケコサシスセソタチツテトナニヌネノハヒフヘホマミムメモヤユヨラリルレロワン゛゜"
kana_dakuten_nashi <- "ウカキクケコサシスセソタチツテトハヒフヘホ"
kana_dakuten_ari <- "ヴガギグゲゴザジズゼゾダヂヅデドバビブベボ"
kana_handakuten_nashi <- "ハヒフヘホ"
kana_handakuten_ari <- "パピプペポ"
zenkaku <- unlist(strsplit(zenkaku, NULL))
hankaku <- unlist(strsplit(hankaku, NULL))
hankana <- unlist(strsplit(hankana, NULL))
zenkaku_kana <- unlist(strsplit(zenkaku_kana, NULL))
kana_dakuten_nashi <- unlist(strsplit(kana_dakuten_nashi, NULL))
kana_dakuten_ari <- unlist(strsplit(kana_dakuten_ari, NULL))
kana_handakuten_nashi <- unlist(strsplit(kana_handakuten_nashi, NULL))
kana_handakuten_ari <- unlist(strsplit(kana_handakuten_ari, NULL))

for (i in names(df)) {
  for (j in 1:length(zenkaku)) {
    df[, i] <- gsub(zenkaku[j], hankaku[j], df[, i])
  }
  for (j in 1:length(hankana)) {
    df[, i] <- gsub(hankana[j], zenkaku_kana[j], df[, i])
  }
  for (j in 1:length(kana_dakuten_nashi)) {
    df[, i] <- gsub(paste0(kana_dakuten_nashi[j], "゛"), kana_dakuten_ari[j], df[, i])
  }
  for (j in 1:length(kana_handakuten_nashi)) {
    df[, i] <- gsub(paste0(kana_handakuten_nashi[j], "゜"), kana_handakuten_ari[j], df[, i])
  }
}
```
濁点と半濁点以外の正規化は，次のコマンドでできるようである（詳細な確認はしていない）。
```{R}
#| eval: false
library(audubon)
audubon::strj_normalize(textdata)
```


## MeCab

最初に，形態素解析エンジン MeCab をパソコンにインストールします。
[MeCab: Yet Another Part-of-Speech and Morphological Analyzer](http://taku910.github.io/mecab/){target="_blank"} から，MeCab本体とMeCab用の辞書（IPA辞書）をダウンロードしてください。
続いて，ダウンロードしたファイルをインストールします。
Windowsの場合，インストール時に辞書の文字コードを選択する画面が表示されます。
迷ったら，URF-8でよいでしょう。
macOSの場合はSourceをダウンロードして，UNIXと同じ方法でインストールしてください。

以下では，Rから MeCab を使うための複数の方法を紹介します。

### テキストデータ

テキスト分析をするためには，テキストデータが必要です。
ここでは，青空文庫からデータを取得します。
```{r}
#| message: false
library(rvest)
library(stringr)
library(dplyr)

process_book <- function(url, bookname){
  temp_file <- paste0(bookname, ".html")
  download.file(url, destfile = temp_file)

  text <- rvest::read_html(temp_file) %>%
    rvest::html_nodes(xpath = "//div[@class='main_text']") %>%
    rvest::html_text()

  unlink(temp_file)

  text <- trimws(text)
  text_norubi <- gsub("（.*?）", "\\1", text)
  paragraphs <- stringr::str_split(text_norubi, "\r\n")
  df <- tibble::tibble(text = unlist(paragraphs), book = bookname)

  return(df)
}

# 坊ちゃん
url <- "https://www.aozora.gr.jp/cards/000148/files/752_14964.html"
botchan_df <- process_book(url, "坊ちゃん")

# 羅生門
url <- "https://www.aozora.gr.jp/cards/000879/files/127_15260.html"
rashomon_df <- process_book(url, "羅生門")

textdata <- bind_rows(botchan_df, rashomon_df)
textdata
```
後ほど，テキストデータをファイルから読み込む場合に対応するために，ここで，テキストファイルとして保存しておきます。
自分のパソコンで以下のコマンドを実行する場合，ファイルパスに注意し，自分の環境に合ったものに変更してください。
```{r}
#| message: false
library(readr)

dirname <- "./data/sample_text"
if (!file.exists(dirname)) {
  dir.create(dirname)
}

readr::write_lines(botchan_df$text, file.path(dirname, "坊ちゃん.txt"))
readr::write_lines(rashomon_df$text, file.path(dirname, "羅生門.txt"))
```

### RMecab

まず，[RMecab](http://rmecab.jp/wiki/){target="_blank"} パッケージを使う方法です。

RMecab は CRAN にはありませんので，次のコマンドでパッケージをインストールします。
```{r}
#| eval: false
install.packages("RMeCab", repos = "http://rmecab.jp/R")
```

正しくインストールできた場合，以下のようにすると，意図した結果が得られます。
```{r}
library(RMeCab)

wagahai <- "吾輩は猫である。名前はまだ無い。"
out <- RMeCabC(wagahai)
unlist(out)
```
正常に動作することが確認できたら，ファイルを読み込んでみます。
```{r}
#| message: false
filename <- file.path(dirname, "坊ちゃん.txt")

text_freq <- RMeCab::RMeCabFreq(filename)
head(text_freq)
unique(text_freq$Info1)
unique(text_freq$Info2)
```
続いて，細かい設定の例を示します。
```{r}
library(dplyr)

sorted_text_freq <- text_freq |>
  dplyr::arrange(desc(Freq)) |>
  filter(!Info1 %in% c("記号", "助詞", "助動詞") |
         !Info2 %in% "非自立")

head(sorted_text_freq)
```

次に，テキストファイルから単語文書行列の作成を試みます。
`docMatrix()` の最初の引数には，テキストファイルのあるディレクトリを指定します。
```{r}
tdm <- RMeCab::docMatrix(dirname, pos = c("名詞", "形容詞", "副詞", "動詞"), weight = "")

tdm <- as.data.frame.matrix(tdm)
tdm <- tdm[rownames(tdm) != "[[LESS-THAN-1]]", , drop = FALSE]
tdm <- tdm[rownames(tdm) != "[[TOTAL-TOKENS]]", , drop = FALSE]
colnames(tdm) <- sub("\\.txt", "", colnames(tdm))

# quoted_rows <- grep("\"", row.names(tdm))
# if (length(quoted_rows) > 0) {
#   tdm <- tdm[-quoted_rows, , drop = FALSE]
# }

tdm <- tdm[order(rowSums(tdm), decreasing = TRUE), , drop = FALSE]
word_freqs <- data.frame(freq = rowSums(tdm))
```

分析結果を可視化する方法として，最も単純なもののひとつにワードクラウドがあります。
```{r}
#| message: false
library(wordcloud)
library(RColorBrewer)

word_freqs_20 <- word_freqs[word_freqs$freq > 19, , drop = FALSE]
word_freqs_top300 <- head(word_freqs, 300)

wordcloud(word = row.names(word_freqs_20), freq = word_freqs_20$freq, random.order = FALSE, rot.per = 0, colors = brewer.pal(8, "Dark2"))
```
一部の目立つ単語は，ストップワードを削除することで対応します。

#### DocumentTermMatrix()

テキスト分析に有益なパッケージとして古くからある [tm](https://CRAN.R-project.org/package=tm){target="_blank"} パッケージでは，`DocumentTermMatrix` オブジェクトが定義されています。
`RMeCab` パッケージのデータを `DocumentTermMatrix` オブジェクトとして扱うには，次のようにします。
```{r}
textdata2 <- textdata %>%
  group_by(book) %>%
  summarise(text = paste0(text, collapse = " ")) %>%
  ungroup()
textdata2

library(tm)

word_list <- RMeCab::RMeCabC(textdata2$text[1])
corp <- tm::Corpus(VectorSource(word_list))
dtm <- tm::DocumentTermMatrix(corp)

dtm

word_freqs <- colSums(as.matrix(dtm))
df <- data.frame(term = names(word_freqs), freq = word_freqs)

df |>
  with(wordcloud(words = term, freq = freq,
                 min.freq = 20, random.order = FALSE, rot.per = 0,
                 colors = brewer.pal(8, "Dark2")))
```

#### MeCabで参照する辞書の追加

macOSの場合，`added.csv` を UTF-8 で作成し，"~/Documents" に保存します。
`Terminal.app` を起動し，次のコードを実行します。
```
$ cd ~/Documents
$ /usr/local/libexec/mecab/mecab-dict-index -d/usr/local/lib/mecab/dic/ipadic -u added.dic -f utf8 -t utf8 added.csv
$ cp /usr/local/etc/mecabrc ~/.mecabrc
$ vi ~/.mecabrc
```
次の1行を入力して，ファイルを保存し，vi を終了します（`:wq!`）。
```
userdic = /Users/username/Documents/added.dic
```

Windowsの場合，`added.csv` を（Shift-JIS）で作成し，"C:\Program Files (x86)\MeCab\dic\ipadic" に保存します。
`cmd.exe` を起動し，次のコードを実行します。
```
> cd C:\Program Files (x86)\MeCab\bin
> mecab-dict-index -d "C:\Program Files (x86)\MeCab\dic\ipadic" -f shift-jis -t shift-jis
```
以下のファイルが生成されます。
```
char.bin
matrix.bin
sys.dic
unk.dic
```
すべて，"C:\Program Files (x86)\MeCab\dic\ipadic" のファイルに置換します。

### RcppMeCab

次に，[RcppMeCab](https://github.com/junhewk/RcppMeCab){target="_blank"} パッケージを使う方法です。

インストールは，CRAN からできます。
ただし，[RでMeCab（RcppMeCab）を利用して形態素解析する方法](https://zenn.dev/paithiov909/articles/4777d371178aa7b98b4e) で説明されているとおり，CRAN の更新は止まっているようです。
開発版の方がいいかもしれません。
```{r}
#| eval: false
install_github("junhewk/RcppMeCab")
```

正しくインストールできた場合，以下のようにすると，意図した結果が得られます。
```{r}
library(RcppMeCab)

out <- RcppMeCab::pos(wagahai, join = FALSE)
RcppMeCab::pos(wagahai, format = "data.frame")
```

保存したファイルから，テキストデータを読み込みます。
```{r}
#| warning: false
filenames <- list.files(dirname, full.names = TRUE)
textdata3 <- list()
for (i in filenames) {
  textdata3[basename(i)] <- paste(readLines(i), collapse = "")
}
textdata3 <- unlist(textdata3)
textdata3
```
これを最近流行の方法で書くと，次のようになります。
```{r}
library(purrr)
library(readr)
library(tidyr)

textdata3 <- tibble::tibble(filenames = list.files(dirname, full.names = TRUE)) |>
  dplyr::mutate(
    .keep = "none", 
    text = purrr::map(filenames, readr::read_lines),
    id = basename(filenames)) |>
  tidyr::unnest(text)
textdata3
```
形態素解析は次のようにやります。
```{r}
text_pos <- RcppMeCab::pos(textdata3$text, format = "data.frame")
text_pos[3:23, ]
```

### gibasa

最後に，[gibasa](https://github.com/paithiov909/gibasa){target="_blank"} パッケージを使う方法です。
使い方は，[RとMeCabによる日本語テキストマイニングの前処理](https://paithiov909.github.io/textmining-ja/){target="_blank"} で詳しく説明されています。

このパッケージには MeCab のソースコードが含まれています。
ただし，辞書は別途用意する必要があるようです。
上記手順で MeCab をインストールしていれば，辞書のインストールと設定は必要ありません。
辞書のパスは以下のコマンドで分かります。
```{r}
#| message: false
library(gibasa)
gibasa::dictionary_info()
```
それっぽいものが返ってくれば，gibasa が使えます。

形態素解析は次のようなコマンドを実行します。
```{r}
gibasa::tokenize(wagahai)
```

ここで，macOSだと次のエラーメッセージとともにエラーが返ってくることがある。
```
 *** caught segfault ***
address 0x9ffffffe7, cause 'invalid permissions'
```
これはメモリの問題で，おそらく CPU が Apple silicon のときに起こる現象のようである。
同じ Apple silicon でも，RStudio だとエラーにならず，処理が完了する。
このコマンドを R で実行するにはどうすべきか，今のところ分からない。

```{r}
textdata4 <- textdata |>
  dplyr::filter(book == "坊ちゃん") |>
  dplyr::mutate(doc_id = seq_along(text) |> as.character()) |>
  dplyr::mutate(text = audubon::strj_normalize(text))

text_token <- gibasa::tokenize(textdata4, text, doc_id)

reactable::reactable(text_token, compact = TRUE)

text_token |>
  gibasa::prettify(col_select = c("POS1", "Original")) |>
  dplyr::filter(POS1 %in% c("名詞", "形容詞", "副詞", "動詞"))
```

トークンをまとめて新しいトークンにすることができる。
```{r}
reactable::reactable(gibasa::prettify(text_token, col_select = c("POS1", "POS2", "POS3")), compact = TRUE)
reactable::reactable(gibasa::prettify(text_token, col_select = c("POS1", "POS2", "POS3")), compact = FALSE)

text_token |>
  gibasa::prettify(col_select = c("POS1", "POS2", "POS3", "Original")) |>
  gibasa::collapse_tokens(
    (POS1 %in% c("名詞", "形容詞", "副詞", "動詞") & !stringr::str_detect(token, "^[あ-ン]+$")) |
    (POS1 %in% c("名詞", "形容詞") & POS2 %in% c("自立", "接尾", "数接続"))
  )
```
これは非常に便利であるが，どうしてこうした結果が得られるのか，今のところ理解できていない。

```{r}
dat_count <- text_token |>
  gibasa::prettify(col_select = c("POS1", "POS2", "POS3", "Original")) |>
  gibasa::collapse_tokens(
    (POS1 %in% c("名詞", "形容詞", "副詞", "動詞") & !stringr::str_detect(token, "^[ァ-ヴ\\s]+$")) |
    (POS1 %in% c("名詞", "形容詞") & POS2 %in% c("自立", "接尾", "数接続"))
  ) |>
  dplyr::mutate(
    doc_id = forcats::fct_drop(doc_id),
    token = token
  ) |>
  dplyr::count(doc_id, token)

dat_count
```

文書単語行列は次のように作成する。
```{r}
#| eval: false
dtm <- dat_count |>
  tidytext::cast_sparse(doc_id, token, n)

dtm[, 1:7]
```


## TreeTagger

[TreeTagger](https://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/){target="_blank"}


[koRpus](https://reaktanz.de/?c=hacking&s=koRpus){target="_blank"}


## tidytext

[tidytext](https://CRAN.R-project.org/package=tidytext){target="_blank"} パッケージはテキスト分析を効率的に行うためのツールを提供します。
このパッケージの作成者らが書いた著書 [Welcome to Text Mining with R](https://www.tidytextmining.com/){target="_blank"} が参考になります。

ここで，以下の説明に使うサンプルテキストを用意しておきます。
```{R}
path <- "../data/sampleEN"
filenames <- list.files(path, full.names = TRUE)
textEN <- list()
for (i in filenames) {
  textEN[[sub(paste0(path, "/"), "", i)]] <- readLines(i)
}
textdata <- paste0(unlist(textEN), collapse = "")
```

### トークン化

テキストをトークンに分割することをトークン化（分かち書き）と呼びます。
このトークン化を実現するものが，[tokenizers](https://CRAN.R-project.org/package=tokenizers){target="_blank"} パッケージです。
トークンとして，単語（words），文字（characters），ステム，センテンス，段落などを指定できます。
```{R}
library(tokenizers)
tokenize_words(textdata)[[1]]
```

テキスト分析では，ストップワードを削除する必要があります。
ストップワードとは，出現頻度が高すぎる動詞などであり，分析に役に立たないばかりか，分析結果をつまらないものにします。
`tidytext` のデフォルトのストップワードは以下に挙げるものです。
```{r}
tidytext::stop_words
```
ストップワードを辞書ごとに `filter()` することも可能である。
```{r}
table(tidytext::stop_words$lexicon)
```
他にストップワードを追加してもよいです。
ストップワードの追加は，`unnest_tokens()` の返り値に対して，次のようにします。
```{r}
text_token <- textdata4 |>
  tidytext::unnest_tokens(output = token, input = text)

mystopwords <- tibble::tibble(token = c("eq", "co", "rc", "ac", "ak", "bn", "fig"))
new_textdata <- anti_join(text_token, mystopwords, by = "token")
```
ただし，次のようにパイプ処理の中で削除するのが，現代的かもしれません。
```{r}
#| eval: false
tidy_books <- dplyr::tibble(text = unlist(textEN))

tidy_books <- tidy_books |>
  dplyr::anti_join(tidytext::stop_words)
```

```{r}
text <- c("This is a pen.", "That are pens.", "This car is white.")

library(dplyr)
library(tidytext)

text_df <- text |>
  dplyr::tibble(line = seq_along(text))
text_df |>
  tidytext::unnest_tokens(word, text)
```
`unnest_tokens()` では句読点，記号は削除され，小文字に変換される。


```{R}
#| eval: false
book_words %>%
  bind_tf_idf(word, book, n)
```

[tm](https://CRAN.R-project.org/package=tm){target="_blank"} パッケージ

### センチメント分析

```{r}
#| eval: false
tidytext::sentiments()
table(tidytext::sentiments$sentiment)
```

### ワードクラウド

```{r}
#| eval: false
tidy_books %>%
  dplyr::anti_join(tidytext::stop_words) %>%
  dplyr::count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```


### 整理テキスト -> DTM

```{r}
#| eval: false
tidytext::tidy()
```

### DTM -> 整理テキスト

```{r}
#| eval: false
tidytext::tidy()
```

## quanteda

[quanteda](https://github.com/quanteda/quanteda){target="_blank"}

文書特徴量行列（document-feature matrix）
```{r}
#| eval: false
quanteda::dfm()
```
