---
project:
  type: website
  output-dir: docs
title: "テキストマイニング"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
format:
  html:
    toc: true
    toc-title: 目次
    toc_float: true
    toc-depth: 4
    number-sections: true
    theme: simplex
    mermaid:
      theme: neutral
lang: ja
---

テキストマイニングは，大雑把には以下のような手順により，テキストデータを量的に分析する方法です。
なお，このページの説明には多くの間違いがある可能性があります。
間違いを指摘いただければ幸いです。

- テキストデータの収集
- テキストデータの読み込み
- 前処理
- 形態素解析
— 単語文書行列（term-document matrix）または文書単語行列（document-term matrix）の作成
- 分析


## 前処理

テキストマイニングでは前処理が重要です。
前処理は，形態素解析の前に行います。

環境によっては文字コードを変換する必要がある場合もあります。
文字コードがUTF-8でない場合にUTF-8に変換するようにすればよいでしょう。
```{R}
#| eval: false
iconv(textdata, from = "CP932", to = "UTF-8")
```

表記揺れの修正は日本語のテキストマイニングにおいて重要でしょう。
例えば，次のようにします。
```{R}
#| eval: false
textdata <- gsub("子供", "子ども", textdata)
```

もう少し汎用的な処理は正規化と呼ばれる。
例えば，全角と半角を統一することが考えられます。
```{R}
#| eval: false
zenkaku <- "０１２３４５６７８９ａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺ"
hankaku <- "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"
hankana <- "｡｢｣､･ｦｧｨｩｪｫｬｭｮｯｰｱｲｳｴｵｶｷｸｹｺｻｼｽｾｿﾀﾁﾂﾃﾄﾅﾆﾇﾈﾉﾊﾋﾌﾍﾎﾏﾐﾑﾒﾓﾔﾕﾖﾗﾘﾙﾚﾛﾜﾝﾞﾟ"
zenkaku_kana <- "。「」、・ヲァィゥェォャュョッーアイウエオカキクケコサシスセソタチツテトナニヌネノハヒフヘホマミムメモヤユヨラリルレロワン゛゜"
kana_dakuten_nashi <- "ウカキクケコサシスセソタチツテトハヒフヘホ"
kana_dakuten_ari <- "ヴガギグゲゴザジズゼゾダヂヅデドバビブベボ"
kana_handakuten_nashi <- "ハヒフヘホ"
kana_handakuten_ari <- "パピプペポ"
zenkaku <- unlist(strsplit(zenkaku, NULL))
hankaku <- unlist(strsplit(hankaku, NULL))
hankana <- unlist(strsplit(hankana, NULL))
zenkaku_kana <- unlist(strsplit(zenkaku_kana, NULL))
kana_dakuten_nashi <- unlist(strsplit(kana_dakuten_nashi, NULL))
kana_dakuten_ari <- unlist(strsplit(kana_dakuten_ari, NULL))
kana_handakuten_nashi <- unlist(strsplit(kana_handakuten_nashi, NULL))
kana_handakuten_ari <- unlist(strsplit(kana_handakuten_ari, NULL))

for (i in names(df)) {
  for (j in 1:length(zenkaku)) {
    df[, i] <- gsub(zenkaku[j], hankaku[j], df[, i])
  }
  for (j in 1:length(hankana)) {
    df[, i] <- gsub(hankana[j], zenkaku_kana[j], df[, i])
  }
  for (j in 1:length(kana_dakuten_nashi)) {
    df[, i] <- gsub(paste0(kana_dakuten_nashi[j], "゛"), kana_dakuten_ari[j], df[, i])
  }
  for (j in 1:length(kana_handakuten_nashi)) {
    df[, i] <- gsub(paste0(kana_handakuten_nashi[j], "゜"), kana_handakuten_ari[j], df[, i])
  }
}
```
濁点と半濁点以外の正規化は，次のコマンドでできるようである（詳細な確認はしていない）。
```{R}
#| eval: false
library(audubon)
audubon::strj_normalize(textdata)
```


## MeCab

最初に，形態素解析エンジン MeCab をパソコンにインストールします。
[MeCab: Yet Another Part-of-Speech and Morphological Analyzer](http://taku910.github.io/mecab/){target="_blank"} から，MeCab本体とMeCab用の辞書（IPA辞書）をダウンロードしてください。
続いて，ダウンロードしたファイルをインストールします。
Windowsの場合，インストール時に辞書の文字コードを選択する画面が表示されます。
迷ったら，URF-8でよいでしょう。
macOSの場合はSourceをダウンロードして，UNIXと同じ方法でインストールしてください。

以下では，Rから MeCab を使う方法を2つ紹介します。

### RMecab

まず，[RMecab](http://rmecab.jp/wiki/){target="_blank"} パッケージを使う方法です。

RMecab は CRAN にはありませんので，次のコマンドでパッケージをインストールします。
```{r}
#| eval: false
install.packages("RMeCab", repos = "http://rmecab.jp/R")
```

正しくインストールできた場合，以下のようにすると，意図した結果が得られます。
```{r}
library(RMeCab)

source("http://rmecab.jp/R/Aozora.R")
x <- Aozora("https://www.aozora.gr.jp/cards/000148/files/752_ruby_2438.zip")
RMeCab::RMeCabFreq(x)

sumomo <- "すもももももももものうち"
res <- RMeCab::RMeCabC(sumomo)
unlist(res)
```
正常に動作することが確認できたら，次に，テキストファイルを読んで，単語文書行列を作成してみます。
`docMatrix()` の最初の引数には，テキストファイルのあるディレクトリを指定します。
```{r}
dirname <- "../data/sampleJP"
tdm <- RMeCab::docMatrix(dirname, pos = c("名詞", "形容詞", "副詞", "動詞"), weight = "")

tdm <- as.data.frame.matrix(tdm)
colnames(tdm) <- sub("\\.txt", "", colnames(tdm))
tdm <- tdm[rownames(tdm) != "[[LESS-THAN-1]]", , drop = FALSE]
tdm <- tdm[rownames(tdm) != "[[TOTAL-TOKENS]]", , drop = FALSE]

quoted_rows <- grep("\"", row.names(tdm))
if (length(quoted_rows) > 0) {
  tdm <- tdm[-quoted_rows, , drop = FALSE]
}

tdm <- tdm[order(rowSums(tdm), decreasing = TRUE), , drop = FALSE]
word_freqs <- data.frame(freq = rowSums(tdm))
```

分析結果を可視化する方法として，最も単純なものとして，ワードクラウドがあります。
```{r}
#| message: false
library(wordcloud)
library(RColorBrewer)

wordcloud(row.names(word_freqs), word_freqs[[1]], random.order = FALSE, rot.per = 0, colors = brewer.pal(8, "Dark2"))
```
一般名詞が目立つのが気になります。
これは，ストップワードを削除することで対応できます。


### RcppMeCab

次に，[RcppMeCab](https://github.com/junhewk/RcppMeCab){target="_blank"} パッケージを使う方法です。

インストールは，CRAN からできます。
ただし，[RでMeCab（RcppMeCab）を利用して形態素解析する方法](https://zenn.dev/paithiov909/articles/4777d371178aa7b98b4e) で説明されているとおり，CRAN の更新は止まっているようです。
開発版の方がいいかもしれません。
```{r}
#| eval: false
install_github("junhewk/RcppMeCab")
```

正しくインストールできた場合，以下のようにすると，意図した結果が得られます。
```{r}
library(RcppMeCab)
res <- RcppMeCab::pos(sumomo, join = FALSE)
res
RcppMeCab::pos(sumomo, format = "data.frame")
```


```{r}
#| warning: false
path <- "../data/sampleJP"
filenames <- list.files(path, full.names = TRUE)
out <- list()
for (i in filenames) {
  out[sub(paste0(path, "/"), "", i)] <- paste(readLines(i), collapse = "")
}
head(RcppMeCab::pos(out[[1]], format = "data.frame"))
```


### gibasa

最後に，[gibasa](https://github.com/paithiov909/gibasa){target="_blank"} パッケージを使う方法です。

このパッケージには MeCab のソースコードが含まれています。
ただし，辞書は別途用意する必要があるようです。
上記手順で MeCab をインストールしていれば，辞書のインストールと設定は必要ありません。
辞書のパスは以下のコマンドで分かります。
```{r}
library(gibasa)
gibasa::dictionary_info()
```
それっぽいものが返ってくれば，gibasa が使えます。

形態素解析は次のようなコマンドを実行します。
```{r}
gibasa::tokenize(sumomo)
```

ここで，macOSだと次のエラーメッセージとともにエラーが返ってくることがある。

 *** caught segfault ***
address 0x9ffffffe7, cause 'invalid permissions'

これはメモリの問題で，おそらく Apple silicon のみ起こる現象のようである。
同じ Apple silicon でも，RStudio だとエラーにならず，処理が完了する。
このコマンドを R で実行するにはどうすべきか，今のところ分からない。

```{r}
dat_txt <-
  tibble::tibble(
    doc_id = seq_along(out) |> as.character(),
    text = out
  ) |>
  dplyr::mutate(text = audubon::strj_normalize(text))

dat <- gibasa::tokenize(dat_txt, text, doc_id)

reactable::reactable(dat, compact = TRUE)

dat |>
  gibasa::prettify(col_select = c("POS1", "Original")) |>
  dplyr::filter(POS1 %in% c("名詞", "形容詞", "副詞", "動詞"))
```

トークンをまとめて新しいトークンにすることができる。
```{r}
reactable::reactable(gibasa::prettify(dat, col_select = c("POS1", "POS2", "POS3")), compact = TRUE)
reactable::reactable(gibasa::prettify(dat, col_select = c("POS1", "POS2", "POS3")), compact = FALSE)

dat |>
  gibasa::prettify(col_select = c("POS1", "POS2", "POS3", "Original")) |>
  gibasa::collapse_tokens(
    (POS1 %in% c("名詞", "形容詞", "副詞", "動詞") & !stringr::str_detect(token, "^[あ-ン]+$")) |
    (POS1 %in% c("名詞", "形容詞") & POS2 %in% c("自立", "接尾", "数接続"))
  )
```
これは非常に便利であるが，どうしてこうした結果が得られるのか，今のところ理解できていない。

```{r}
dat_count <- dat |>
  gibasa::prettify(col_select = c("POS1", "POS2", "POS3", "Original")) |>
  gibasa::collapse_tokens(
    (POS1 %in% c("名詞", "形容詞", "副詞", "動詞") & !stringr::str_detect(token, "^[ァ-ヴ\\s]+$")) |
    (POS1 %in% c("名詞", "形容詞") & POS2 %in% c("自立", "接尾", "数接続"))
  ) |>
  dplyr::mutate(
    doc_id = forcats::fct_drop(doc_id),
    token = token
  ) |>
  dplyr::count(doc_id, token)

dat_count
```

文書単語行列は次のように作成する。
```{r}
dtm <- dat_count |>
  tidytext::cast_sparse(doc_id, token, n)

dtm[, 1:7]
```


## TreeTagger

[TreeTagger](https://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/){target="_blank"}


[koRpus](https://reaktanz.de/?c=hacking&s=koRpus){target="_blank"}


## tidytext

[tidytext](https://CRAN.R-project.org/package=tidytext){target="_blank"} パッケージはテキストマイニングを効率的に行うためのツールを提供します。
このパッケージの作成者らが書いた著書 [Welcome to Text Mining with R](https://www.tidytextmining.com/){target="_blank"} が参考になります。


[tokenizers](https://CRAN.R-project.org/package=tokenizers){target="_blank"} パッケージ＝トークン化（テキストをトークンに分割すること。トークンはデフォルトで単語（words），その他に，文字（characters），センテンス，段落など。）

テキスト分析では，ストップワードを削除する必要がある。
ストップワードとは，出現頻度が高すぎる動詞などであり，分析に役に立たないばかりか，分析結果をつまらないものにします。
```{r}
tidytext::stop_words
```
ストップワードを辞書ごとに `filter()` することも可能である。
```{r}
table(tidytext::stop_words$lexicon)
```
他にストップワードを追加してもよいです。
ただし，`mutate(word = stringr::str_extract(word, "[a-z]+"))` のように，`dplyr` のパイプ処理の際に削除するのが，現代的かもしれません。

```{r}
tidy_books <- tidy_books %>% 
  dplyr::anti_join(tidytext::stop_words)
```

```{r}
text <- c("Because I could not stop for Death -", "He kindly stopped for me -",
"The Carriage held but just Ourselves -", "and Immortality")

library(dplyr)
library(tidytext)

text_df <- dplyr::tibble(line = 1:4, text = text)
text_df |> tidytext::unnest_tokens(word, text)
```
`unnest_tokens()` では句読点，記号は削除され，小文字に変換される。

[tm](https://CRAN.R-project.org/package=tm){target="_blank"} パッケージ

### センチメント分析

```{r}
tidytext::sentiments()
table(tidytext::sentiments$sentiment)
```

### ワードクラウド

```{r}
library(wordcloud)

tidy_books %>%
  dplyr::anti_join(tidytext::stop_words) %>%
  dplyr::count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

## quanteda

[quanteda](https://github.com/quanteda/quanteda){target="_blank"}

