---
project:
  type: website
  output-dir: docs
title: "テキストマイニング"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
format:
  html:
    toc: true
    toc-title: 目次
    toc_float: true
    toc-depth: 4
    number-sections: true
    theme: simplex
    mermaid:
      theme: neutral
lang: ja
---

テキストマイニングは，大雑把には以下のような手順により，テキストデータを量的に分析する方法です。

- テキストデータの収集
- テキストデータの読み込み
- 前処理
- 形態素解析
— 単語文書行列（term-document matrix）の作成
- 分析


## 前処理

テキストマイニングでは前処理が重要です。
前処理は，RMeCabで処理する前に行います。
例えば，全角と半角を統一することが考えられます。
```{R}
zenkaku <- "０１２３４５６７８９ａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺ"
hankaku <- "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"
hankana <- "｡｢｣､･ｦｧｨｩｪｫｬｭｮｯｰｱｲｳｴｵｶｷｸｹｺｻｼｽｾｿﾀﾁﾂﾃﾄﾅﾆﾇﾈﾉﾊﾋﾌﾍﾎﾏﾐﾑﾒﾓﾔﾕﾖﾗﾘﾙﾚﾛﾜﾝﾞﾟ"
zenkaku_kana <- "。「」、・ヲァィゥェォャュョッーアイウエオカキクケコサシスセソタチツテトナニヌネノハヒフヘホマミムメモヤユヨラリルレロワン゛゜"
kana_dakuten_nashi <- "ウカキクケコサシスセソタチツテトハヒフヘホ"
kana_dakuten_ari <- "ヴガギグゲゴザジズゼゾダヂヅデドバビブベボ"
kana_handakuten_nashi <- "ハヒフヘホ"
kana_handakuten_ari <- "パピプペポ"
zenkaku <- unlist(strsplit(zenkaku, NULL))
hankaku <- unlist(strsplit(hankaku, NULL))
hankana <- unlist(strsplit(hankana, NULL))
zenkaku_kana <- unlist(strsplit(zenkaku_kana, NULL))
kana_dakuten_nashi <- unlist(strsplit(kana_dakuten_nashi, NULL))
kana_dakuten_ari <- unlist(strsplit(kana_dakuten_ari, NULL))
kana_handakuten_nashi <- unlist(strsplit(kana_handakuten_nashi, NULL))
kana_handakuten_ari <- unlist(strsplit(kana_handakuten_ari, NULL))

for (i in names(df)) {
  for (j in 1:length(zenkaku)) {
    df[, i] <- gsub(zenkaku[j], hankaku[j], df[, i])
  }
  for (j in 1:length(hankana)) {
    df[, i] <- gsub(hankana[j], zenkaku_kana[j], df[, i])
  }
  for (j in 1:length(kana_dakuten_nashi)) {
    df[, i] <- gsub(paste0(kana_dakuten_nashi[j], "゛"), kana_dakuten_ari[j], df[, i])
  }
  for (j in 1:length(kana_handakuten_nashi)) {
    df[, i] <- gsub(paste0(kana_handakuten_nashi[j], "゜"), kana_handakuten_ari[j], df[, i])
  }
}
```
また，表記揺れを修正したい場合は，次のようにすればよいでしょう。
```{R}
gsub("子供", "子ども", textdata)
```

環境によっては文字コードを変換する必要がある場合もあります。
文字コードがUTF-8でない場合にUTF-8に変換するようにすればよいでしょう。
```{R}
iconv(textdata, from = "CP932", to = "UTF-8")
```


## MeCab

最初に，形態素解析エンジン MeCab をパソコンにインストールします。
[MeCab: Yet Another Part-of-Speech and Morphological Analyzer](http://taku910.github.io/mecab/){target="_blank"} から，MeCab本体とMeCab用の辞書（IPA辞書）をダウンロードしてください。
続いて，ダウンロードしたファイルをインストールします。
macOSの場合はSourceをダウンロードして，UNIXと同じ方法でインストールしてください。

以下では，Rから MeCab を使う方法を2つ紹介します。

### RMecab

まず，[RMecab](http://rmecab.jp/wiki/){target="_blank"} パッケージを使う方法です。

RMecab は CRAN にはありませんので，次のコマンドでパッケージをインストールします。
```{r}
#| eval: false
install.packages("RMeCab", repos = "http://rmecab.jp/R")
```

正しくインストールできた場合，以下のようにすると，意図した結果が得られます。
```{r}
library(RMeCab)

sumomo <- "すもももももももものうち"
res <- RMeCab::RMeCabC(sumomo)
unlist(res)
```
正常に動作することが確認できたら，次に，テキストファイルを読んで，単語文書行列を作成してみます。
`docMatrix()` の最初の引数には，テキストファイルのあるディレクトリを指定します。
```{r}
filename <- "../data/sampleJP"
tdm <- RMeCab::docMatrix(filename, pos = c("名詞", "形容詞", "副詞", "動詞"), weight = "")

tdmatrix <- as.data.frame.matrix(tdm)
colnames(tdmatrix) <- sub("\\.txt", "", colnames(tdmatrix))

tdmatrix <- tdmatrix[rownames(tdmatrix) != "[[LESS-THAN-1]]", , drop = FALSE]
tdmatrix <- tdmatrix[rownames(tdmatrix) != "[[TOTAL-TOKENS]]", , drop = FALSE]

quoted_rows <- grep("\"", row.names(tdmatrix))
if (length(quoted_rows) > 0) {
  tdmatrix <- tdmatrix[-quoted_rows, , drop = FALSE]
}

tdmatrix <- tdmatrix[order(rowSums(tdmatrix), decreasing = TRUE), , drop = FALSE]
word_freqs <- data.frame(freq = rowSums(tdmatrix))
```

分析結果を可視化する方法として，最も単純なものとして，ワードクラウドがあります。
```{r}
library(wordcloud)
library(RColorBrewer)

wordcloud(row.names(word_freqs), word_freqs[[1]], random.order = FALSE, rot.per = 0, colors = brewer.pal(8, "Dark2"))
```
一般名詞が目立つのが気になります。
これは，ストップワードを削除することで対応できます。


### RcppMeCab

次に，[RcppMeCab](https://github.com/junhewk/RcppMeCab){target="_blank"} パッケージを使う方法です。

インストールは，CRAN からできます。
ただし，[RでMeCab（RcppMeCab）を利用して形態素解析する方法](https://zenn.dev/paithiov909/articles/4777d371178aa7b98b4e) で説明されているとおり，CRAN の更新は止まっているようです。
開発版の方がいいかもしれません。
```{r}
install_github("junhewk/RcppMeCab")
```

正しくインストールできた場合，以下のようにすると，意図した結果が得られます。
```{r}
library(RcppMeCab)
res <- RcppMeCab::pos(sumomo, join = FALSE)
res
RcppMeCab::pos(sumomo, format = "data.frame")
```


```{r}
#| warning: false
path <- "../data/sampleJP"
filenames <- list.files(path, full.names = TRUE)
out <- list()
for (i in filenames) {
  out[sub(paste0(path, "/"), "", i)] <- paste(readLines(i), collapse = "")
}
head(RcppMeCab::pos(out[[1]], format = "data.frame"))
```


## gibasa

最後に，[gibasa](https://github.com/paithiov909/gibasa){target="_blank"} パッケージを使う方法です。

このパッケージには MeCab のソースコードが含まれています。
ただし，辞書は別途用意する必要があるようです。
ここでは，上記手順で MeCab をインストールしていることを前提としており，辞書のインストールと設定の方法は説明しません。
辞書のパスは以下のコマンドで分かります。
```{r}
library(gibasa)
gibasa::dictionary_info()
```
それっぽいものが返ってくれば，gibasa が使えます。


```{r}
gibasa::tokenize(sumomo)
gibasa::tokenize(c("頭が赤い魚を食べた猫", "望遠鏡で泳ぐ彼女を見た"))
```


## TreeTagger

[TreeTagger](https://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/){target="_blank"}


[koRpus](https://reaktanz.de/?c=hacking&s=koRpus){target="_blank"}


## tidytext

[tidytext](https://cran.r-project.org/web/packages/tidytext/index.html){target="_blank"}

[Welcome to Text Mining with R](https://www.tidytextmining.com/){target="_blank"}


## quanteda

[quanteda](https://github.com/quanteda/quanteda){target="_blank"}

